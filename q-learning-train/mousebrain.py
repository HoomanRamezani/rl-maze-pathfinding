# -*- coding: utf-8 -*-
"""scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16Nigm-5F4B7TvALFG7luY00IDVWHvz4W
"""

# Commented out IPython magic to ensure Python compatibility.
from __future__ import print_function
import datetime, json, random
import numpy as np
from keras.models import Sequential
from keras.layers.core import Dense
from keras.optimizers import SGD, Adam, RMSprop
from keras.layers import PReLU, LeakyReLU
import matplotlib.pyplot as plt
from random import randint

# %matplotlib inline


visited_mark = 0.8  # Cells visited by the mouse will be painted by gray 0.8
mouse_indicator = 0.5  # The current mouse cell will be painteg by gray 0.5
MOVE_LEFT = 0
MOVE_UP = 1
MOVE_RIGHT = 2
MOVE_DOWN = 3

# Actions dictionary
mouse_actions_dict = {
    MOVE_LEFT: "left",
    MOVE_UP: "up",
    MOVE_RIGHT: "right",
    MOVE_DOWN: "down",
}

num_actions: int = len(mouse_actions_dict)

# Exploration factor
# explore 10% of the time
mouse_exploration_factor = 0.1


# Class to define structure of a node
class Node:
    def __init__(self, value=None, next_element=None):
        self.val = value
        self.next = next_element


# Class to implement a stack
class Stack:
    # Constructor
    def __init__(self):
        self.head = None
        self.length = 0

    # Put an item on the top of the stack
    def insert(self, data):
        self.head = Node(data, self.head)
        self.length += 1

    # Return the top position of the stack
    def pop(self):
        if self.length == 0:
            return None
        else:
            returned = self.head.val
            self.head = self.head.next
            self.length -= 1
            return returned

    # Return False if the stack is empty
    # and true otherwise
    def not_empty(self):
        return bool(self.length)

    # Return the top position of the stack
    def top(self):
        return self.head.val


def generate_random_maze(
    rows: int = 8,
    columns: int = 8,
    initial_point: list = (0, 0),
    final_point: list = (7, 7),
):
    ROWS, COLS = rows, columns

    # Array with only walls (where paths will
    # be created)
    maze = list(list(0 for _ in range(COLS)) for _ in range(ROWS))

    # Auxiliary matrices to avoid cycles
    seen = list(list(False for _ in range(COLS)) for _ in range(ROWS))
    previous = list(list((-1, -1) for _ in range(COLS)) for _ in range(ROWS))

    S = Stack()

    # Insert initial position
    S.insert(initial_point)

    # Keep walking on the graph using dfs
    # until we have no more paths to traverse
    # (create)
    while S.not_empty():
        # Remove the position of the Stack
        # and mark it as seen
        x, y = S.pop()
        seen[x][y] = True

        # This is to avoid cycles with adj positions
        if (x + 1 < ROWS) and maze[x + 1][y] == 1 and previous[x][y] != (x + 1, y):
            continue
        if (0 < x) and maze[x - 1][y] == 1 and previous[x][y] != (x - 1, y):
            continue
        if (y + 1 < COLS) and maze[x][y + 1] == 1 and previous[x][y] != (x, y + 1):
            continue
        if (y > 0) and maze[x][y - 1] == 1 and previous[x][y] != (x, y - 1):
            continue

        # Mark as walkable position
        maze[x][y] = 1

        # Array to shuffle neighbours before
        # insertion
        to_stack = []

        # Before inserting any position,
        # check if it is in the boundaries of
        # the maze
        # and if it were seen (to avoid cycles)

        # If adj position is valid and was not seen yet
        if (x + 1 < ROWS) and seen[x + 1][y] == False:
            # Mark the adj position as seen
            seen[x + 1][y] = True

            # Memorize the position to insert the
            # position in the stack
            to_stack.append((x + 1, y))

            # Memorize the current position as its
            # previous position on the path
            previous[x + 1][y] = (x, y)

        if (0 < x) and seen[x - 1][y] == False:
            # Mark the adj position as seen
            seen[x - 1][y] = True

            # Memorize the position to insert the
            # position in the stack
            to_stack.append((x - 1, y))

            # Memorize the current position as its
            # previous position on the path
            previous[x - 1][y] = (x, y)

        if (y + 1 < COLS) and seen[x][y + 1] == False:
            # Mark the adj position as seen
            seen[x][y + 1] = True

            # Memorize the position to insert the
            # position in the stack
            to_stack.append((x, y + 1))

            # Memorize the current position as its
            # previous position on the path
            previous[x][y + 1] = (x, y)

        if (y > 0) and seen[x][y - 1] == False:
            # Mark the adj position as seen
            seen[x][y - 1] = True

            # Memorize the position to insert the
            # position in the stack
            to_stack.append((x, y - 1))

            # Memorize the current position as its
            # previous position on the path
            previous[x][y - 1] = (x, y)

        # Indicates if Pf is a neighbour position
        pf_flag = False
        while len(to_stack):
            # Remove random position
            neighbour = to_stack.pop(randint(0, len(to_stack) - 1))

            # Is the final position,
            # remember that by marking the flag
            if neighbour == final_point:
                pf_flag = True

            # Put on the top of the stack
            else:
                S.insert(neighbour)

        # This way, Pf will be on the top
        if pf_flag:
            S.insert(final_point)

    # Mark the initial position
    x0, y0 = initial_point
    xf, yf = final_point
    maze[x0][y0] = 1
    maze[xf][yf] = 1

    # Return maze formed by the traversed path
    return np.asarray(maze, dtype="float")


# maze is a 2d Numpy array of floats between 0.0 to 1.0
# 1.0 corresponds to a free cell, and 0.0 an occupied cell
# mouse = (row, col) initial mouse position (defaults to (0,0))


class Qmaze(object):
    def __init__(self, maze: np.ndarray, mouse=(0, 0)):
        self._maze: np.ndarray = np.array(maze)
        nrows, ncols = self._maze.shape
        # the target cheese cell is always the bottom corner of the maze
        # target cell where the "cheese" is
        self.target = (nrows - 1, ncols - 1)
        self.free_cells = [
            (r, c)
            for r in range(nrows)
            for c in range(ncols)
            if self._maze[r, c] == 1.0
        ]
        self.free_cells.remove(self.target)
        if self._maze[self.target] == 0.0:
            raise Exception("Invalid maze: target cell cannot be blocked!")
        if not mouse in self.free_cells:
            raise Exception("Invalid Mouse Location: must sit on a free cell")
        self.reset(mouse)

    def reset(self, mouse):
        self.mouse = mouse
        self.maze: np.ndarray = np.copy(self._maze)
        nrows, ncols = self.maze.shape
        row, col = mouse
        self.maze[row, col] = mouse_indicator
        self.state = (row, col, "start")
        self.min_reward = -0.5 * self.maze.size
        self.total_reward = 0
        self.visited = set()

    def update_state(self, action: int):
        nrows, ncols = self.maze.shape
        nrow, ncol, nmode = mouse_row, mouse_col, mode = self.state

        if self.maze[mouse_row, mouse_col] > 0.0:
            self.visited.add((mouse_row, mouse_col))  # mark visited cell

        valid_actions = self.valid_actions()

        if not valid_actions:
            nmode = "blocked"
        elif action in valid_actions:
            nmode = "valid"
            if action == MOVE_LEFT:
                ncol -= 1
            elif action == MOVE_UP:
                nrow -= 1
            if action == MOVE_RIGHT:
                ncol += 1
            elif action == MOVE_DOWN:
                nrow += 1
        else:  # invalid action, no change in mouse position
            nmode = "invalid"

        # update new state
        self.state = (nrow, ncol, nmode)

    def get_reward(self):
        mouse_row, mouse_col, mode = self.state
        nrows, ncols = self.maze.shape
        # if the mouse is at the same location of the cheese, return the max reward
        if mouse_row == nrows - 1 and mouse_col == ncols - 1:
            return 1.0
        # if the mouse cannot remove, return the mininum reward
        if mode == "blocked":
            return self.min_reward - 1
        # return a -0.25 penalty if the region has already been visited by the mouse
        if (mouse_row, mouse_col) in self.visited:
            return -0.25
        # return -0.75 for an attempted invalid move
        if mode == "invalid":
            return -0.75
        # return a -0.04 penalty for moving around to promote more direct route to cheese.
        if mode == "valid":
            return -0.04

    def act(self, action: int):
        self.update_state(action)
        reward = self.get_reward()
        self.total_reward += reward
        status = self.trail_status()
        envstate = self.observe()
        return envstate, reward, status

    def observe(self):
        """
        Returns the current state of the Maze
        """
        canvas: np.ndarray = self.draw_maze()
        envstate = canvas.reshape((1, -1))
        return envstate

    def draw_maze(self):
        """
        Draws the maze canvas
        """
        canvas: np.ndarray = np.copy(self.maze)
        nrows, ncols = self.maze.shape
        # clear all visual marks
        for r in range(nrows):
            for c in range(ncols):
                if canvas[r, c] > 0.0:
                    canvas[r, c] = 1.0
        # draw the mouse
        row, col, valid = self.state
        canvas[row, col] = mouse_indicator
        return canvas

    def trail_status(self):
        """
        Gets the trial status
        """
        if self.total_reward < self.min_reward:
            return "lose"
        mouse_row, mouse_col, mode = self.state
        nrows, ncols = self.maze.shape
        if mouse_row == nrows - 1 and mouse_col == ncols - 1:
            return "win"

        return "not_over"

    def valid_actions(self, cell=None):
        """
        Determines if the selected action by the mouse is valid
        """
        if cell is None:
            row, col, mode = self.state
        else:
            row, col = cell
        actions = [0, 1, 2, 3]
        nrows, ncols = self.maze.shape
        if row == 0:
            actions.remove(1)
        elif row == nrows - 1:
            actions.remove(3)
        if col == 0:
            actions.remove(0)
        elif col == ncols - 1:
            actions.remove(2)
        if row > 0 and self.maze[row - 1, col] == 0.0:
            actions.remove(1)
        if row < nrows - 1 and self.maze[row + 1, col] == 0.0:
            actions.remove(3)
        if col > 0 and self.maze[row, col - 1] == 0.0:
            actions.remove(0)
        if col < ncols - 1 and self.maze[row, col + 1] == 0.0:
            actions.remove(2)
        return actions


def show(qmaze: Qmaze):
    """
    Renders the maze
    """
    print("IN")
    plt.grid("on")
    nrows, ncols = qmaze.maze.shape
    ax = plt.gca()
    ax.set_xticks(np.arange(0.5, nrows, 1))
    ax.set_yticks(np.arange(0.5, ncols, 1))
    ax.set_xticklabels([])
    ax.set_yticklabels([])
    canvas = np.copy(qmaze.maze)
    for row, col in qmaze.visited:
        canvas[row, col] = 0.6
    mouse_row, mouse_col, _ = qmaze.state
    canvas[mouse_row, mouse_col] = 0.3  # mouse cell
    canvas[nrows - 1, ncols - 1] = 0.9  # cheese cell
    img = plt.imsave("test.png", canvas, cmap="gray")
    return img


maze = generate_random_maze()

qmaze = Qmaze(maze)


def run_trial(model: Sequential, qmaze: Qmaze, mouse_cell):
    """
    Runs a trial with a given mode, a maze, and the mouse's initial position
    """
    qmaze.reset(mouse_cell)
    envstate = qmaze.observe()
    while True:
        prev_envstate = envstate
        # get next action
        q = model.predict(prev_envstate)
        action = np.argmax(q[0])

        # apply action, get rewards and new state
        envstate, reward, game_status = qmaze.act(action)
        if game_status == "win":
            return True
        elif game_status == "lose":
            return False


def completion_check(model, qmaze: Qmaze):
    for cell in qmaze.free_cells:
        if not qmaze.valid_actions(cell):
            return False
        if not run_trial(model, qmaze, cell):
            return False
    return True


class Experience(object):
    """
    `max_memory` is set to `30` according to the following source:
     Recent and remote spatial memory in mice treated with cytosine arabinoside
     DOI:10.1016/j.pbb.2011.10.008
     
    `discount=0.95` less than once to force convergence and the discounted sum reward
     rather than the long term reward
    """

    def __init__(self, model: Sequential, max_memory=30, discount=0.95):
        self.model = model
        self.max_memory = max_memory
        self.discount = discount
        self.memory: np.ndarray = list()
        self.num_actions = model.output_shape[-1]

    def remember(self, trial):
        # trial = [envstate, action, reward, envstate_next, game_over]
        # memory[i] = episode
        # envstate == flattened 1d maze cells info, including mouse cell (see method: observe)
        self.memory.append(trial)
        if len(self.memory) > self.max_memory:
            del self.memory[0]

    def predict(self, envstate):
        return self.model.predict(envstate)[0]

    def get_data(self, data_size=8):
        env_size = self.memory[0][0].shape[1]  # envstate 1d size (1st element of trial)
        mem_size = len(self.memory)
        data_size = min(mem_size, data_size)
        inputs = np.zeros((data_size, env_size))
        targets = np.zeros((data_size, self.num_actions))
        for idx, jdx in enumerate(
            np.random.choice(range(mem_size), data_size, replace=False)
        ):
            envstate, action, reward, envstate_next, game_over = self.memory[jdx]
            inputs[idx] = envstate
            # There should be no target values for actions not taken.
            targets[idx] = self.predict(envstate)
            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')
            Q_sa = np.max(self.predict(envstate_next))
            if game_over:
                targets[idx, action] = reward
            else:
                # reward + gamma * max_a' Q(s', a')
                targets[idx, action] = reward + self.discount * Q_sa
        return inputs, targets


# Commented out IPython magic to ensure Python compatibility.
def qtrain(model: Sequential, maze, **opt):
    global mouse_exploration_factor
    n_epoch = opt.get("epochs", 200)
    max_memory = opt.get("max_memory", 30)
    data_size = opt.get("data_size", 8)
    weights_file = opt.get("weights_file", "")
    name = opt.get("name", "model")
    start_time = datetime.datetime.now()

    # If you want to continue training from a previous model,
    # just supply the h5 file name to weights_file option
    if weights_file:
        print("loading weights from file: %s" % (weights_file,))
        model.load_weights(weights_file)

    # Construct environment/game from numpy array: maze (see above)
    qmaze = Qmaze(maze)

    # Initialize experience replay object
    experience = Experience(model, max_memory=max_memory)

    win_history = []  # history of win/lose game
    n_free_cells = len(qmaze.free_cells)
    hsize = qmaze.maze.size // 2  # history window size
    win_rate = 0.0
    imctr = 1

    for epoch in range(n_epoch):
        loss = 0.0
        mouse_cell = random.choice(qmaze.free_cells)
        qmaze.reset(mouse_cell)
        trial_over = False

        # get initial envstate (1d flattened canvas)
        envstate = qmaze.observe()

        n_trials = 0
        while not trial_over:
            valid_actions = qmaze.valid_actions()
            if not valid_actions:
                break
            prev_envstate = envstate
            # Get next action
            if np.random.rand() < mouse_exploration_factor:
                action = random.choice(valid_actions)
            else:
                action = np.argmax(experience.predict(prev_envstate))

            # Apply action, get reward and new envstate
            envstate, reward, game_status = qmaze.act(action)
            if game_status == "win":
                win_history.append(1)
                trial_over = True
            elif game_status == "lose":
                win_history.append(0)
                trial_over = True
            else:
                trial_over = False

            # Store episode (experience)
            trial = [prev_envstate, action, reward, envstate, trial_over]
            experience.remember(trial)
            n_trials += 1

            # Train neural network model
            inputs, targets = experience.get_data(data_size=data_size)
            h = model.fit(inputs, targets, nb_epoch=8, batch_size=16, verbose=0,)
            loss = model.evaluate(inputs, targets, verbose=0)

        if len(win_history) > hsize:
            win_rate = sum(win_history[-hsize:]) / hsize

        dt = datetime.datetime.now() - start_time
        t = format_time(dt.total_seconds())
        template = "Epoch: {:03d}/{:d} | Loss: {:.4f} | Trials: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}"
        print(
            template.format(
                epoch, n_epoch - 1, loss, n_trials, sum(win_history), win_rate, t
            )
        )
        # we simply check if training has exhausted all free cells and if in all
        # cases the agent won
        if win_rate > 0.9:
            mouse_exploration_factor = 0.05
        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):
            print("Reached 100%% win rate at epoch: %d" % (epoch,))
            break

    # Save trained model weights and architecture, this will be used by the visualization code
    h5file = name + ".h5"
    json_file = name + ".json"
    model.save_weights(h5file, overwrite=True)
    with open(json_file, "w") as outfile:
        json.dump(model.to_json(), outfile)
    end_time = datetime.datetime.now()
    dt = datetime.datetime.now() - start_time
    seconds = dt.total_seconds()
    t = format_time(seconds)
    print("files: %s, %s" % (h5file, json_file))
    print(
        "n_epoch: %d, max_mem: %d, data: %d, time: %s"
        #         % (epoch, max_memory, data_size, t)
    )
    return seconds


# This is a small utility for printing readable time strings:
def format_time(seconds):
    if seconds < 400:
        s = float(seconds)
        return "%.1f seconds" % (s,)
    elif seconds < 4000:
        m = seconds / 60.0
        return "%.2f minutes" % (m,)
    else:
        h = seconds / 3600.0
        return "%.2f hours" % (h,)


def mouse_brain(maze: np.ndarray, lr=0.001):
    """
    Mouse Brain Model
    """
    """
    create a sequential model that we will add layers to
    """
    model: Sequential = Sequential()
    """
    Create a first layer that serves as the visual cortex,
    receiving input based on what the mouse can see
    """
    model.add(Dense(maze.size, input_shape=(maze.size,), activation="relu"))

    """
    Dense set of Leaky ReLU neurons representing cortex
    """
    model.add(Dense(maze.size))
    model.add(PReLU())

    """
    Bottle neck excite and inhibit neurons in BG
    """
    model.add(Dense(maze.size / 4))
    model.add(PReLU())
    model.add(Dense(maze.size))
    model.add(PReLU())

    """
    the final layer of the model is a dense later of neurons
    whos output is the the dimension of the number of actions
    when predicting, the output of the model is an action
    this can be thought of as the motor cortex, which synapse directly
    onto motor neurons
    """
    model.add(Dense(num_actions))
    model.compile(optimizer="adam", loss="mse")
    return model


maze = generate_random_maze()
qmaze = Qmaze(maze)
model = mouse_brain(maze)
qtrain(model, maze, epochs=500, max_memory=30, data_size=8)

